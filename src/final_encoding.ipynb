{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f2a6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param to run the encoding pipeline on all subjects or on 1 subject (faster)\n",
    "batch_mode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf08f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets dataframes with 140 time points and cuts the excess\n",
    "# 140 is the minimum \n",
    "\n",
    "import os \n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "folder_path = \"abide_timeseries/aal_csv\" # can also be cc200\n",
    "\n",
    "subjects = dict()\n",
    "\n",
    "# subject_dfs = []\n",
    "\n",
    "df = pd.read_csv(\"abide_timeseries/phenotypic.csv\")\n",
    "label_dict = dict(zip(df[\"SUB_ID\"].astype(str), df[\"DX_GROUP\"]))\n",
    "\n",
    "for index, filename in enumerate(sorted(os.listdir(folder_path))):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            df = df.drop(index=0).reset_index(drop=True) # skips second row which is metadata\n",
    "\n",
    "            if len(df) >= 140:\n",
    "                match = re.search(r'00\\d+', filename)\n",
    "                \n",
    "                if match:\n",
    "                    subj_id = str(int(match.group()))  # remove leading zeros\n",
    "\n",
    "                    trimmed_df = df.iloc[:140]\n",
    "                    # subject_dfs.append(trimmed_df)\n",
    "\n",
    "                    subjects[subj_id] = trimmed_df\n",
    "            \n",
    "            if not batch_mode:\n",
    "                if index == 20:\n",
    "                    break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {filename}: {e}\")\n",
    "\n",
    "print(\"finished getting subject data frames (140 timepoints)\")\n",
    "print(f\"subject_dfs length: {len(subjects.items())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba97f3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect one item from the `subjects` dict\n",
    "try:\n",
    "    first_key = next(iter(subjects))\n",
    "    first_value = subjects[first_key]\n",
    "    print(f\"First subject key: {first_key}\")\n",
    "    if hasattr(first_value, \"shape\"):\n",
    "        print(f\"Value type: {type(first_value)} shape={first_value.shape}\")\n",
    "    else:\n",
    "        print(f\"Value type: {type(first_value)}\")\n",
    "    print(\"Preview:\")\n",
    "    try:\n",
    "        print(first_value.head())\n",
    "    except Exception:\n",
    "        print(first_value)\n",
    "except StopIteration:\n",
    "    print(\"subjects is empty (build it in the earlier cell first)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23536308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get windows per subject df, total per subject is 11 slices/windows\n",
    "subject_df_windows = dict() # 2d arr [[...windows], [...windows]]\n",
    "\n",
    "for subj_id, df in subjects.items():\n",
    "    windows = []\n",
    "    for start in range (0, 140-40 + 1, 10): # step 10 with 40 length\n",
    "        window = df.iloc[start:start + 40].copy()\n",
    "        windows.append(window)\n",
    "    \n",
    "    subject_df_windows[subj_id] = windows\n",
    "\n",
    "print(\"finished getting slices from dfs\")\n",
    "\n",
    "avg_windows_length = 0\n",
    "for df_windows in subject_df_windows.values():\n",
    "    avg_windows_length += len(df_windows)\n",
    "\n",
    "print(f\"avg windows per df: {avg_windows_length/len(subject_df_windows)}\\n\")\n",
    "\n",
    "print(\"sample window from first df:\")\n",
    "print(next(iter(next(iter(subject_df_windows.values())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d178125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "subject_fcs = dict() # 2d arr [[...fc matrices]]\n",
    "\n",
    "for subj_id, subject_df in subject_df_windows.items():\n",
    "    subject_fc_matrices = []\n",
    "    for df_window in subject_df:\n",
    "        fc_matrix = df_window.corr(method='pearson')\n",
    "        fc_matrix = fc_matrix.fillna(0) # replaces w 0 null values (zero connectivity assumption)\n",
    "        subject_fc_matrices.append(fc_matrix)\n",
    "    \n",
    "    subject_fcs[subj_id] = subject_fc_matrices\n",
    "\n",
    "print(\"finished getting functional connectivity matrices\")\n",
    "\n",
    "avg_fc_matrix_length = 0\n",
    "for fc in subject_fcs.values():\n",
    "    avg_fc_matrix_length += len(fc)\n",
    "\n",
    "print(f\"avg fc matrices per subj: {avg_fc_matrix_length/len(subject_fcs.values())}\\n\")\n",
    "\n",
    "print(\"sample first fc matrix from first subject:\")\n",
    "print(next(iter(next(iter(subject_fcs.values())))))\n",
    "\n",
    "print(\"visualization of fc matrix:\")\n",
    "sns.heatmap(next(iter(next(iter(subject_fcs.values())))), cmap='coolwarm', center=0)\n",
    "plt.title(\"Subject 0 - Window 0 FC Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8f9c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edges for the GCN (global z-score across subjects)\n",
    "import numpy as np\n",
    "\n",
    "subject_adjacency_matrices = dict()\n",
    "subject_order = []\n",
    "\n",
    "for subj_id, subject_fc in subject_fcs.items():\n",
    "    stacked_fcs = np.stack(subject_fc)\n",
    "    summed_fc_values = np.sum(stacked_fcs, axis=0)\n",
    "    fc_strength_per_region = summed_fc_values / stacked_fcs.shape[0]\n",
    "\n",
    "    x = stacked_fcs[:-1, :, :]\n",
    "    y = stacked_fcs[1:, :, :]\n",
    "    cov = np.mean((x - np.mean(x, axis=0)) * (y - np.mean(y, axis=0)), axis=0)\n",
    "    std_x = np.std(x, axis=0)\n",
    "    std_y = np.std(y, axis=0)\n",
    "    eps = 1e-8\n",
    "    fc_stability_per_region = cov / (std_x * std_y + eps)\n",
    "\n",
    "    # Raw weights per subject (no per-subject minâ€“max)\n",
    "    adj_matrix = (fc_strength_per_region + fc_stability_per_region) / 2\n",
    "\n",
    "    np.fill_diagonal(adj_matrix, 0)\n",
    "\n",
    "    if np.isnan(adj_matrix).any():\n",
    "        print('has nan for subject', subj_id)\n",
    "\n",
    "    subject_adjacency_matrices[subj_id] = adj_matrix\n",
    "    subject_order.append(subj_id)\n",
    "\n",
    "print(\"finished computing raw adjacency matrices per subject\")\n",
    "\n",
    "# Stack all adjacency matrices: shape [S, N, N]\n",
    "all_adj = np.stack([subject_adjacency_matrices[sid] for sid in subject_order], axis=0)\n",
    "\n",
    "# Global z-score per edge using shared mean/std across subjects\n",
    "global_mean = np.mean(all_adj, axis=0)\n",
    "global_std = np.std(all_adj, axis=0)\n",
    "eps_z = 1e-8\n",
    "all_adj_z = (all_adj - global_mean) / (global_std + eps_z)\n",
    "\n",
    "# Map back into dict\n",
    "for i, sid in enumerate(subject_order):\n",
    "    subject_adjacency_matrices[sid] = all_adj_z[i]\n",
    "\n",
    "print(\"applied global z-score normalization across subjects [S,N,N]\")\n",
    "\n",
    "print(\"sample adjacency matrix from first subject (z-scored):\")\n",
    "print(next(iter(subject_adjacency_matrices.values())))\n",
    "\n",
    "print(\"visualization of adjacency matrix (global z-score):\")\n",
    "sns.heatmap(next(iter(subject_adjacency_matrices.values())), cmap='coolwarm', center=0)\n",
    "plt.title(\"Subject 0 - Adjacency Matrix (global z-score)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9662ec",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "np.save(\"gcn_input/subject_adjacency_matrices.npy\", np.array(list(subject_adjacency_matrices.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6c362f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed the fc matrices to the random walker plainly\n",
    "# each jump is decided randomly first either interlayer or intralayer (weighted on how strong the intralayer connections are)\n",
    "import random\n",
    "\n",
    "def random_walk(matrices, num_walks=200, walk_length=100, threshold=0.7):\n",
    "    walks = []\n",
    "    for _ in range(num_walks):\n",
    "        m = matrices[0].shape[0]\n",
    "        t = np.random.randint(0, len(matrices))\n",
    "        i = np.random.randint(0, m) # random region\n",
    "        walk = []\n",
    "\n",
    "        for _ in range(walk_length):\n",
    "            region_id = i\n",
    "            walk.append(region_id)\n",
    "\n",
    "            if t == len(matrices) - 1:\n",
    "                break\n",
    "                \n",
    "            current_matrix = matrices[t]\n",
    "            strong_neighbours = []\n",
    "            current_region_connections = current_matrix.iloc[i].values\n",
    "            for index, conn in enumerate(current_region_connections):\n",
    "                if conn > threshold and region_id != index:\n",
    "                    strong_neighbours.append(index)\n",
    "\n",
    "            if strong_neighbours:\n",
    "                i = random.choice(strong_neighbours)\n",
    "            else:\n",
    "                t += 1\n",
    "\n",
    "        walks.append(walk)\n",
    "    return walks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee644901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes for the GCN (global, label-aware Word2Vec + per-subject fine-tune)\n",
    "from gensim.models import Word2Vec\n",
    "from copy import deepcopy\n",
    "\n",
    "subject_embedding_matrices = dict()\n",
    "subject_ids = []\n",
    "\n",
    "# 1) Build global corpus with label tokens\n",
    "#    - Each subject's walk becomes a sentence of ROI ids as strings\n",
    "#    - We inject a label token so the embedding learns class-aware co-occurrences\n",
    "#      ABIDE DX_GROUP: 1=ASD, 2=Control (map others conservatively)\n",
    "\n",
    "def _label_token_for_subject(subj_id: str) -> str:\n",
    "    raw = label_dict.get(str(subj_id))\n",
    "    if raw in (1, \"1\"):\n",
    "        return \"__LABEL_ASD__\"\n",
    "    if raw in (2, \"2\"):\n",
    "        return \"__LABEL_CTL__\"\n",
    "    # Fallback for any unexpected value\n",
    "    return f\"__LABEL_{raw}__\"\n",
    "\n",
    "# Cache subject-specific sentences to avoid regenerating walks twice\n",
    "subject_sentences = dict()\n",
    "global_sentences = []\n",
    "\n",
    "print(\"Building global corpus for Word2Vec (with label tokens)...\")\n",
    "for subj_id, subject_fc in subject_fcs.items():\n",
    "    subj_label_tok = _label_token_for_subject(subj_id)\n",
    "    subject_walks = random_walk(subject_fc)\n",
    "\n",
    "    # Insert label token once per sentence (at both ends to increase interactions)\n",
    "    sentences = []\n",
    "    for walk in subject_walks:\n",
    "        tokens = [str(n) for n in walk]\n",
    "        if len(tokens) == 0:\n",
    "            continue\n",
    "        sentences.append([subj_label_tok] + tokens + [subj_label_tok])\n",
    "\n",
    "    subject_sentences[subj_id] = sentences\n",
    "    global_sentences.extend(sentences)\n",
    "\n",
    "print(f\"Total sentences in global corpus: {len(global_sentences)}\")\n",
    "\n",
    "# 2) Train a single global model (shared across subjects) using labels\n",
    "global_model = Word2Vec(\n",
    "    sentences=global_sentences,\n",
    "    vector_size=128,\n",
    "    window=2,\n",
    "    min_count=0,\n",
    "    sg=1,\n",
    "    workers=4,\n",
    "    epochs=10,\n",
    "    negative=5,\n",
    ")\n",
    "\n",
    "# 3) Per-subject fine-tuning to get subject-specific node embeddings\n",
    "#    We start from the global weights, then adapt on that subject's sentences.\n",
    "#    Finally, we extract per-ROI embeddings (116 x 128) for the GCN.\n",
    "\n",
    "print(\"Fine-tuning per subject and extracting node embeddings...\")\n",
    "for subj_id, sentences in subject_sentences.items():\n",
    "    print(f\"subject: {subj_id}\")\n",
    "\n",
    "    # Make a subject-specific copy and fine-tune briefly\n",
    "    model_subj = deepcopy(global_model)\n",
    "    if len(sentences) > 0:\n",
    "        model_subj.train(sentences, total_examples=len(sentences), epochs=5)\n",
    "\n",
    "    # Build embedding matrix for ROI tokens '0'..'115'\n",
    "    embedding_matrix = np.zeros((116, 128), dtype=np.float32)\n",
    "    for roi_idx in range(116):\n",
    "        token = str(roi_idx)\n",
    "        if token in model_subj.wv:\n",
    "            embedding_matrix[roi_idx] = model_subj.wv[token]\n",
    "        else:\n",
    "            # If somehow OOV, keep zeros for this ROI (should be rare)\n",
    "            embedding_matrix[roi_idx] = 0.0\n",
    "\n",
    "    subject_embedding_matrices[subj_id] = embedding_matrix\n",
    "\n",
    "# 4) Save outputs for downstream\n",
    "np.save(\"gcn_input/subject_embedding_matrices.npy\", np.array(list(subject_embedding_matrices.values())))\n",
    "np.save(\"gcn_input/subject_ids.npy\", np.array(list(subject_embedding_matrices.keys())))\n",
    "\n",
    "print(next(iter(subject_adjacency_matrices.values())).shape)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
